{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "basedir = '../'\n",
    "sys.path.append(basedir)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from synth_data import HldaDataGenerator\n",
    "from hlda.sampler import NCRPNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data test for hierarchical LDA inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['w0' 'w1' 'w2' 'w3' 'w4']\n",
      " ['w5' 'w6' 'w7' 'w8' 'w9']\n",
      " ['w10' 'w11' 'w12' 'w13' 'w14']\n",
      " ['w15' 'w16' 'w17' 'w18' 'w19']\n",
      " ['w20' 'w21' 'w22' 'w23' 'w24']\n",
      " ['w25' 'w26' 'w27' 'w28' 'w29']\n",
      " ['w30' 'w31' 'w32' 'w33' 'w34']\n",
      " ['w35' 'w36' 'w37' 'w38' 'w39']\n",
      " ['w40' 'w41' 'w42' 'w43' 'w44']\n",
      " ['w45' 'w46' 'w47' 'w48' 'w49']\n",
      " ['w50' 'w51' 'w52' 'w53' 'w54']\n",
      " ['w55' 'w56' 'w57' 'w58' 'w59']\n",
      " ['w60' 'w61' 'w62' 'w63' 'w64']\n",
      " ['w65' 'w66' 'w67' 'w68' 'w69']\n",
      " ['w70' 'w71' 'w72' 'w73' 'w74']\n",
      " ['w75' 'w76' 'w77' 'w78' 'w79']\n",
      " ['w80' 'w81' 'w82' 'w83' 'w84']\n",
      " ['w85' 'w86' 'w87' 'w88' 'w89']\n",
      " ['w90' 'w91' 'w92' 'w93' 'w94']\n",
      " ['w95' 'w96' 'w97' 'w98' 'w99']]\n"
     ]
    }
   ],
   "source": [
    "n_rows = 20\n",
    "n_cols = 5\n",
    "vocab_mat = np.zeros((n_rows, n_cols), dtype=np.object)\n",
    "word_count = 0\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        vocab_mat[i, j] = 'w%s' % word_count\n",
    "        word_count += 1\n",
    "        \n",
    "print vocab_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w0', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10', 'w11', 'w12', 'w13', 'w14', 'w15', 'w16', 'w17', 'w18', 'w19', 'w20', 'w21', 'w22', 'w23', 'w24', 'w25', 'w26', 'w27', 'w28', 'w29', 'w30', 'w31', 'w32', 'w33', 'w34', 'w35', 'w36', 'w37', 'w38', 'w39', 'w40', 'w41', 'w42', 'w43', 'w44', 'w45', 'w46', 'w47', 'w48', 'w49', 'w50', 'w51', 'w52', 'w53', 'w54', 'w55', 'w56', 'w57', 'w58', 'w59', 'w60', 'w61', 'w62', 'w63', 'w64', 'w65', 'w66', 'w67', 'w68', 'w69', 'w70', 'w71', 'w72', 'w73', 'w74', 'w75', 'w76', 'w77', 'w78', 'w79', 'w80', 'w81', 'w82', 'w83', 'w84', 'w85', 'w86', 'w87', 'w88', 'w89', 'w90', 'w91', 'w92', 'w93', 'w94', 'w95', 'w96', 'w97', 'w98', 'w99']\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab_mat.flatten().tolist()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assign Documents to Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "node 0 (level=0, documents=100): \n",
      "    node 1 (level=1, documents=97): \n",
      "        node 2 (level=2, documents=42): \n",
      "        node 3 (level=2, documents=22): \n",
      "        node 4 (level=2, documents=30): \n",
      "        node 5 (level=2, documents=2): \n",
      "        node 9 (level=2, documents=1): \n",
      "    node 6 (level=1, documents=3): \n",
      "        node 7 (level=2, documents=2): \n",
      "        node 8 (level=2, documents=1): \n"
     ]
    }
   ],
   "source": [
    "NCRPNode.total_nodes = 0\n",
    "NCRPNode.last_node_id = 0\n",
    "num_levels = 3\n",
    "gamma = 1\n",
    "num_docs = 100\n",
    "\n",
    "root_node = NCRPNode(num_levels, vocab)\n",
    "document_path = {}\n",
    "unique_nodes = set()\n",
    "unique_nodes.add(root_node)\n",
    "for d in range(num_docs):\n",
    "\n",
    "    # populate nodes into the path of this document\n",
    "    path = np.zeros(num_levels, dtype=np.object)\n",
    "    path[0] = root_node\n",
    "    root_node.customers += 1 # always add to the root node first\n",
    "    for level in range(1, num_levels):\n",
    "        # at each level, a node is selected by its parent node based on the CRP prior\n",
    "        parent_node = path[level-1]\n",
    "        level_node = parent_node.select(gamma)\n",
    "        level_node.customers += 1\n",
    "        path[level] = level_node\n",
    "        unique_nodes.add(level_node)\n",
    "\n",
    "    # set the leaf node for this document                 \n",
    "    document_path[d] = path\n",
    "    \n",
    "unique_nodes = sorted(unique_nodes, key=lambda x: x.node_id)\n",
    "print len(unique_nodes)\n",
    "    \n",
    "def print_node(node, indent, node_topic):\n",
    "    out = '    ' * indent\n",
    "    out += 'node %d (level=%d, documents=%d): ' % (node.node_id, node.level, node.customers)\n",
    "    if node in node_topic:\n",
    "        probs, words = node_topic[node]\n",
    "        out += ' '.join(words)\n",
    "    print out        \n",
    "    for child in node.children:\n",
    "        print_node(child, indent+1, node_topic)        \n",
    "\n",
    "node_topic = {}\n",
    "print_node(root_node, 0, node_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Assign Each Node Along the Tree to a Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07877895  0.06869983  0.03838261  0.06514887  0.00435754  0.06984148\n",
      "  0.03655306  0.01200188  0.17818882  0.02428382  0.00138845  0.01088763\n",
      "  0.01633268  0.05047367  0.03911636  0.0899958   0.0893677   0.0110616\n",
      "  0.04605451  0.06908474]\n",
      "['w0' 'w5' 'w10' 'w15' 'w20' 'w25' 'w30' 'w35' 'w40' 'w45' 'w50' 'w55'\n",
      " 'w60' 'w65' 'w70' 'w75' 'w80' 'w85' 'w90' 'w95']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def get_words(vocab_mat, eta, pos, dim):\n",
    "\n",
    "    if dim == 'row':\n",
    "        words = vocab_mat[pos]\n",
    "    elif dim == 'col':\n",
    "        words = vocab_mat[:, pos]\n",
    "    \n",
    "    k = len(words)\n",
    "    eta = [eta] * k\n",
    "    probs = np.random.dirichlet(eta)\n",
    "    return probs, words\n",
    "    \n",
    "pos = 0\n",
    "eta = 1\n",
    "probs, words = get_words(vocab_mat, eta, pos, 'col')\n",
    "print probs\n",
    "print words\n",
    "print np.sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "node_topic = {}\n",
    "node_topic[unique_nodes[0]] = get_words(vocab_mat, eta, 0, 'row') \n",
    "node_topic[unique_nodes[1]] = get_words(vocab_mat, eta, 1, 'row') \n",
    "node_topic[unique_nodes[2]] = get_words(vocab_mat, eta, 2, 'row') \n",
    "node_topic[unique_nodes[3]] = get_words(vocab_mat, eta, 3, 'row') \n",
    "node_topic[unique_nodes[4]] = get_words(vocab_mat, eta, 4, 'row') \n",
    "node_topic[unique_nodes[5]] = get_words(vocab_mat, eta, 5, 'row') \n",
    "node_topic[unique_nodes[6]] = get_words(vocab_mat, eta, 6, 'row') \n",
    "node_topic[unique_nodes[7]] = get_words(vocab_mat, eta, 7, 'row') \n",
    "node_topic[unique_nodes[8]] = get_words(vocab_mat, eta, 8, 'row') \n",
    "node_topic[unique_nodes[9]] = get_words(vocab_mat, eta, 9, 'row') \n",
    "print len(node_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 0 (level=0, documents=100): w0 w1 w2 w3 w4\n",
      "    node 1 (level=1, documents=97): w5 w6 w7 w8 w9\n",
      "        node 2 (level=2, documents=42): w10 w11 w12 w13 w14\n",
      "        node 3 (level=2, documents=22): w15 w16 w17 w18 w19\n",
      "        node 4 (level=2, documents=30): w20 w21 w22 w23 w24\n",
      "        node 5 (level=2, documents=2): w25 w26 w27 w28 w29\n",
      "        node 9 (level=2, documents=1): w45 w46 w47 w48 w49\n",
      "    node 6 (level=1, documents=3): w30 w31 w32 w33 w34\n",
      "        node 7 (level=2, documents=2): w35 w36 w37 w38 w39\n",
      "        node 8 (level=2, documents=1): w40 w41 w42 w43 w44\n"
     ]
    }
   ],
   "source": [
    "print_node(root_node, 0, node_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Words in a Document Based on Its Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_document(topics, theta, doc_len):\n",
    "\n",
    "    # for every word in the vocab for this document\n",
    "    doc = []\n",
    "    for n in range(doc_len):\n",
    "\n",
    "        # sample a new topic index    \n",
    "        k = np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "        # sample a new word from the word distribution of topic k\n",
    "        probs, words = topics[k]\n",
    "        w = np.random.multinomial(1, probs).argmax()\n",
    "        doc_word = words[w]\n",
    "\n",
    "        doc.append(doc_word)\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "# alpha = [2.0, 1.0, 0.5]\n",
    "alpha = [1.0, 1.0, 1.0]\n",
    "doc_len = 50\n",
    "for d in range(num_docs):\n",
    "    path = document_path[d]\n",
    "    topics = [node_topic[node] for node in path]\n",
    "    theta = np.random.mtrand.dirichlet(alpha)\n",
    "    doc = generate_document(topics, theta, doc_len)\n",
    "    corpus.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "outdir = '/Users/joewandy/Dropbox/Analysis/hLDA/data/synthetic/'\n",
    "for d in range(len(corpus)):\n",
    "    doc = corpus[d]\n",
    "    file_name = 'doc_%d.txt' % d\n",
    "    file_path = os.path.join(outdir, file_name)\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"%s\\n\" % ' '.join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run hLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 50\n"
     ]
    }
   ],
   "source": [
    "print len(vocab), len(corpus), len(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert corpus words into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "for doc in corpus:\n",
    "    new_doc = []\n",
    "    for word in doc:\n",
    "        word_idx = vocab.index(word)\n",
    "        new_doc.append(word_idx)\n",
    "    new_corpus.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "['w0', 'w8', 'w13', 'w0', 'w13', 'w10', 'w3', 'w0', 'w0', 'w11', 'w5', 'w11', 'w11', 'w1', 'w11', 'w0', 'w3', 'w0', 'w13', 'w14', 'w3', 'w13', 'w3', 'w3', 'w5', 'w3', 'w3', 'w1', 'w1', 'w0', 'w11', 'w1', 'w3', 'w11', 'w12', 'w3', 'w3', 'w1', 'w3', 'w11', 'w3', 'w5', 'w0', 'w0', 'w12', 'w13', 'w13', 'w8', 'w1', 'w11']\n",
      "[0, 8, 13, 0, 13, 10, 3, 0, 0, 11, 5, 11, 11, 1, 11, 0, 3, 0, 13, 14, 3, 13, 3, 3, 5, 3, 3, 1, 1, 0, 11, 1, 3, 11, 12, 3, 3, 1, 3, 11, 3, 5, 0, 0, 12, 13, 13, 8, 1, 11]\n"
     ]
    }
   ],
   "source": [
    "print len(vocab), len(new_corpus)\n",
    "print corpus[0]\n",
    "print new_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hlda.sampler import HierarchicalLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0] 1 1\n"
     ]
    }
   ],
   "source": [
    "print alpha, gamma, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HierarchicalLDA sampling\n",
      ".......... 10\n",
      "topic 0 (level=0, total_words=1741, documents=100): w1, w3, w0, w4, w8, w13, w7, w5, w11, w14, \n",
      "    topic 1 (level=1, total_words=1581, documents=96): w5, w8, w7, w6, w9, w3, w4, w1, w21, w25, \n",
      "        topic 2 (level=2, total_words=750, documents=42): w11, w13, w14, w10, w12, w5, w7, w8, w1, w6, \n",
      "        topic 3 (level=2, total_words=429, documents=27): w22, w21, w24, w23, w20, w1, w3, w0, w4, w5, \n",
      "        topic 6 (level=2, total_words=300, documents=19): w17, w15, w19, w16, w18, w7, w4, w9, w6, w8, \n",
      "        topic 16 (level=2, total_words=30, documents=1): w25, w29, w27, w26, w28, w36, w30, w31, w32, w33, \n",
      "        topic 18 (level=2, total_words=28, documents=4): w49, w47, w46, w1, w4, w45, w39, w37, w34, w27, \n",
      "        topic 19 (level=2, total_words=32, documents=3): w29, w25, w27, w28, w24, w21, w20, w26, w41, w40, \n",
      "    topic 7 (level=1, total_words=46, documents=4): w7, w8, w5, w6, w1, w3, w9, w33, w36, w35, \n",
      "        topic 20 (level=2, total_words=41, documents=2): w34, w43, w32, w42, w40, w0, w31, w3, w30, w41, \n",
      "        topic 21 (level=2, total_words=22, documents=2): w39, w38, w37, w31, w0, w1, w45, w44, w27, w28, \n",
      "\n",
      ".......... 20\n",
      "topic 0 (level=0, total_words=1785, documents=100): w1, w3, w0, w4, w5, w8, w7, w2, w6, w14, \n",
      "    topic 1 (level=1, total_words=1666, documents=100): w5, w8, w7, w6, w9, w4, w3, w0, w15, w11, \n",
      "        topic 2 (level=2, total_words=665, documents=38): w11, w13, w14, w10, w12, w3, w1, w7, w0, w5, \n",
      "        topic 3 (level=2, total_words=397, documents=29): w22, w21, w24, w23, w20, w5, w0, w3, w6, w8, \n",
      "        topic 6 (level=2, total_words=314, documents=22): w17, w15, w19, w16, w7, w18, w9, w5, w1, w0, \n",
      "        topic 19 (level=2, total_words=79, documents=5): w25, w29, w27, w28, w26, w5, w6, w8, w0, w2, \n",
      "        topic 31 (level=2, total_words=25, documents=2): w39, w38, w37, w4, w31, w34, w36, w27, w28, w29, \n",
      "        topic 32 (level=2, total_words=41, documents=3): w34, w43, w32, w42, w40, w0, w31, w30, w41, w3, \n",
      "        topic 33 (level=2, total_words=28, documents=1): w49, w47, w46, w1, w8, w45, w35, w27, w28, w29, \n",
      "\n",
      ".......... 30\n",
      "topic 0 (level=0, total_words=1738, documents=100): w1, w3, w0, w4, w7, w2, w5, w8, w9, w12, \n",
      "    topic 1 (level=1, total_words=1692, documents=100): w5, w8, w7, w6, w9, w4, w3, w1, w12, w15, \n",
      "        topic 2 (level=2, total_words=695, documents=39): w11, w13, w14, w10, w12, w3, w8, w5, w7, w1, \n",
      "        topic 3 (level=2, total_words=409, documents=30): w22, w21, w24, w23, w20, w5, w0, w8, w3, w9, \n",
      "        topic 6 (level=2, total_words=307, documents=23): w17, w15, w19, w16, w7, w18, w9, w0, w3, w1, \n",
      "        topic 19 (level=2, total_words=73, documents=3): w25, w29, w27, w28, w26, w2, w6, w36, w30, w31, \n",
      "        topic 41 (level=2, total_words=49, documents=4): w49, w39, w47, w38, w37, w46, w4, w5, w31, w34, \n",
      "        topic 43 (level=2, total_words=37, documents=1): w34, w43, w32, w42, w40, w31, w30, w41, w1, w33, \n",
      "\n",
      ".......... 40\n",
      "topic 0 (level=0, total_words=1686, documents=100): w1, w3, w0, w4, w8, w7, w2, w5, w14, w6, \n",
      "    topic 1 (level=1, total_words=1751, documents=100): w5, w8, w7, w6, w9, w3, w1, w4, w0, w13, \n",
      "        topic 2 (level=2, total_words=670, documents=40): w11, w13, w14, w10, w12, w1, w5, w0, w3, w8, \n",
      "        topic 3 (level=2, total_words=408, documents=28): w22, w21, w24, w23, w20, w5, w6, w3, w4, w1, \n",
      "        topic 6 (level=2, total_words=316, documents=23): w17, w15, w19, w16, w18, w7, w1, w4, w9, w6, \n",
      "        topic 19 (level=2, total_words=79, documents=4): w25, w29, w27, w28, w26, w5, w6, w2, w4, w8, \n",
      "        topic 41 (level=2, total_words=52, documents=4): w49, w47, w39, w38, w37, w46, w4, w3, w34, w31, \n",
      "        topic 49 (level=2, total_words=38, documents=1): w34, w43, w32, w42, w40, w31, w30, w41, w1, w3, \n",
      "\n",
      ".......... 50\n",
      "topic 0 (level=0, total_words=1716, documents=100): w1, w3, w0, w4, w5, w2, w7, w9, w10, w17, \n",
      "    topic 1 (level=1, total_words=1713, documents=100): w5, w8, w7, w6, w9, w1, w3, w4, w14, w23, \n",
      "        topic 2 (level=2, total_words=703, documents=41): w11, w13, w14, w10, w12, w3, w8, w1, w0, w5, \n",
      "        topic 3 (level=2, total_words=394, documents=28): w22, w21, w24, w23, w20, w1, w5, w3, w0, w6, \n",
      "        topic 6 (level=2, total_words=311, documents=23): w15, w17, w19, w16, w18, w7, w1, w5, w22, w8, \n",
      "        topic 19 (level=2, total_words=76, documents=4): w25, w29, w27, w28, w26, w0, w5, w6, w1, w2, \n",
      "        topic 58 (level=2, total_words=51, documents=3): w49, w47, w39, w38, w37, w46, w1, w8, w45, w7, \n",
      "        topic 60 (level=2, total_words=36, documents=1): w34, w43, w32, w42, w40, w31, w41, w30, w33, w99, \n",
      "\n",
      ".......... 60\n",
      "topic 0 (level=0, total_words=1693, documents=100): w1, w3, w0, w4, w5, w7, w8, w2, w13, w17, \n",
      "    topic 1 (level=1, total_words=1719, documents=100): w5, w8, w7, w6, w9, w4, w3, w1, w10, w0, \n",
      "        topic 2 (level=2, total_words=703, documents=40): w11, w13, w14, w10, w12, w3, w1, w8, w0, w2, \n",
      "        topic 3 (level=2, total_words=409, documents=28): w22, w21, w24, w23, w20, w5, w3, w8, w9, w6, \n",
      "        topic 6 (level=2, total_words=313, documents=21): w15, w17, w19, w16, w18, w7, w1, w3, w9, w6, \n",
      "        topic 19 (level=2, total_words=76, documents=4): w25, w29, w27, w28, w26, w6, w2, w5, w10, w36, \n",
      "        topic 64 (level=2, total_words=26, documents=2): w39, w38, w37, w0, w31, w45, w34, w26, w27, w28, \n",
      "        topic 67 (level=2, total_words=25, documents=1): w47, w49, w46, w1, w3, w45, w35, w27, w28, w29, \n",
      "        topic 69 (level=2, total_words=0, documents=2): w99, w36, w26, w27, w28, w29, w30, w31, w32, w33, \n",
      "        topic 70 (level=2, total_words=36, documents=2): w34, w43, w32, w42, w40, w31, w41, w30, w33, w99, \n",
      "\n",
      ".......... 70\n",
      "topic 0 (level=0, total_words=1691, documents=100): w1, w3, w0, w4, w5, w8, w2, w9, w16, w7, \n",
      "    topic 1 (level=1, total_words=1692, documents=100): w5, w8, w7, w6, w9, w1, w3, w4, w10, w0, \n",
      "        topic 2 (level=2, total_words=724, documents=40): w11, w13, w14, w10, w12, w3, w1, w5, w8, w6, \n",
      "        topic 3 (level=2, total_words=415, documents=29): w22, w21, w24, w23, w20, w3, w6, w8, w5, w4, \n",
      "        topic 6 (level=2, total_words=316, documents=21): w15, w17, w19, w16, w7, w18, w1, w0, w4, w9, \n",
      "        topic 19 (level=2, total_words=73, documents=2): w25, w29, w27, w28, w26, w2, w6, w36, w30, w31, \n",
      "        topic 64 (level=2, total_words=21, documents=2): w39, w38, w37, w31, w35, w26, w27, w28, w29, w30, \n",
      "        topic 75 (level=2, total_words=29, documents=4): w49, w47, w46, w1, w45, w7, w4, w3, w36, w28, \n",
      "        topic 76 (level=2, total_words=39, documents=2): w34, w43, w32, w42, w40, w31, w4, w30, w41, w3, \n",
      "\n",
      ".......... 80\n",
      "topic 0 (level=0, total_words=1762, documents=100): w1, w3, w0, w4, w5, w7, w2, w9, w6, w8, \n",
      "    topic 1 (level=1, total_words=1683, documents=100): w5, w8, w7, w6, w9, w3, w0, w1, w21, w14, \n",
      "        topic 2 (level=2, total_words=703, documents=40): w11, w13, w14, w10, w12, w1, w8, w5, w3, w4, \n",
      "        topic 3 (level=2, total_words=385, documents=31): w22, w21, w24, w23, w20, w3, w8, w9, w6, w4, \n",
      "        topic 6 (level=2, total_words=302, documents=20): w15, w17, w19, w16, w18, w1, w3, w7, w9, w6, \n",
      "        topic 19 (level=2, total_words=79, documents=3): w25, w29, w27, w28, w26, w8, w6, w7, w0, w2, \n",
      "        topic 77 (level=2, total_words=49, documents=5): w49, w39, w47, w38, w37, w46, w3, w5, w31, w45, \n",
      "        topic 82 (level=2, total_words=37, documents=1): w34, w43, w32, w42, w40, w31, w30, w41, w4, w33, \n",
      "\n",
      ".......... 90\n",
      "topic 0 (level=0, total_words=1705, documents=100): w1, w3, w0, w4, w5, w8, w2, w6, w39, w12, \n",
      "    topic 1 (level=1, total_words=1721, documents=100): w5, w8, w7, w6, w9, w3, w0, w4, w1, w17, \n",
      "        topic 2 (level=2, total_words=690, documents=41): w11, w13, w14, w10, w12, w1, w3, w8, w9, w0, \n",
      "        topic 3 (level=2, total_words=407, documents=28): w22, w21, w24, w23, w20, w3, w0, w1, w7, w8, \n",
      "        topic 6 (level=2, total_words=310, documents=23): w15, w17, w19, w16, w18, w1, w9, w7, w4, w6, \n",
      "        topic 19 (level=2, total_words=75, documents=3): w25, w29, w27, w28, w26, w7, w8, w0, w2, w43, \n",
      "        topic 77 (level=2, total_words=24, documents=2): w38, w39, w37, w4, w31, w0, w44, w34, w27, w28, \n",
      "        topic 88 (level=2, total_words=25, documents=1): w47, w49, w1, w46, w45, w8, w35, w27, w28, w29, \n",
      "        topic 91 (level=2, total_words=43, documents=2): w34, w43, w32, w42, w40, w0, w31, w3, w4, w30, \n",
      "\n",
      ".......... 100\n",
      "topic 0 (level=0, total_words=1608, documents=100): w1, w3, w0, w4, w5, w8, w2, w6, w13, w22, \n",
      "    topic 1 (level=1, total_words=1827, documents=100): w5, w8, w7, w6, w3, w9, w1, w4, w0, w14, \n",
      "        topic 2 (level=2, total_words=691, documents=38): w11, w13, w14, w10, w12, w3, w1, w0, w8, w4, \n",
      "        topic 3 (level=2, total_words=392, documents=29): w22, w21, w24, w23, w20, w5, w8, w0, w6, w1, \n",
      "        topic 6 (level=2, total_words=310, documents=19): w17, w15, w19, w16, w18, w7, w6, w9, w3, w0, \n",
      "        topic 19 (level=2, total_words=79, documents=7): w25, w29, w27, w28, w26, w6, w8, w5, w7, w2, \n",
      "        topic 77 (level=2, total_words=25, documents=2): w39, w38, w37, w31, w34, w35, w26, w27, w28, w29, \n",
      "        topic 97 (level=2, total_words=27, documents=3): w49, w47, w46, w1, w45, w8, w35, w27, w28, w29, \n",
      "        topic 98 (level=2, total_words=41, documents=2): w34, w43, w32, w42, w40, w31, w0, w3, w30, w41, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "hlda = HierarchicalLDA(new_corpus, vocab, alpha=1, gamma=1.0, eta=1.0, num_levels=3)\n",
    "hlda.estimate(n_samples, display_topics=10, n_words=10, with_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
